{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPRVq3e03c1K"
   },
   "source": [
    "# Conversational UI Chatbot App with ChatGPT, LangChain and Chainlit\n",
    "\n",
    "Here we will build a advanced ChatGPT Conversational UI-based chatbot using LangChain and Chainlit with the following features:\n",
    "\n",
    "- Custom Landing Page\n",
    "- Conversational memory\n",
    "- Result streaming capabilities (Real-time output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L1KvMtf54l0d"
   },
   "source": [
    "## Install App and LLM dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain==0.3.11\n",
    "!pip install langchain-openai==0.2.12\n",
    "!pip install chainlit==1.3.2\n",
    "!pip install pyngrok==7.2.2\n",
    "pip install pydantic==2.10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CiwGjVWK4q6F"
   },
   "source": [
    "## Load OpenAI API Credentials\n",
    "\n",
    "Here we load it from a file so we don't explore the credentials on the internet by mistake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5e1HqI56y7t3"
   },
   "outputs": [],
   "source": [
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryheOZuXxa41"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('chatgpt_api_credentials.yml', 'r') as file:\n",
    "    api_creds = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eZs7ts6NzADJ",
    "outputId": "26403aa4-c892-46e3-8a24-f7566bbfc094"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['openai_key'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api_creds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kDe44J0N0NcC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = api_creds['openai_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCMshwB1U9iQ"
   },
   "source": [
    "## Write the app code here and store it in a py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XXceMDF0Qza0",
    "outputId": "4b78011e-51e3-41e6-ae01-111bd6da9dd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "# the following line is a magic command\n",
    "# that will write all the code below it to the python file app.py\n",
    "# we will then deploy this app.py file on the cloud server where colab is running\n",
    "# if you have your own server you can just write the code in app.py and deploy it directly\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain.schema.runnable.config import RunnableConfig\n",
    "from langchain.schema import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "# this function is called when the app starts for the first time\n",
    "async def when_chat_starts():\n",
    "\n",
    "  # Load a connection to ChatGPT LLM\n",
    "  load_dotenv('/home/santhosh/Projects/courses/Pinnacle/.env')\n",
    "  chatgpt = ChatOpenAI(model_name='gpt-4o-mini', temperature=0.1,\n",
    "                       streaming=True)\n",
    "\n",
    "  # Add a basic system prompt for LLM behavior\n",
    "  SYS_PROMPT = \"\"\"\n",
    "               Act as a helpful assistant and answer questions to the best of your ability.\n",
    "               Do not make up answers.\n",
    "               \"\"\"\n",
    "\n",
    "  # Create a prompt template for langchain to use history to answer user prompts\n",
    "  prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "      (\"system\", SYS_PROMPT),\n",
    "      MessagesPlaceholder(variable_name=\"history\"),\n",
    "      (\"human\", \"{input}\"),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "  # Create a memory object to store conversation history window\n",
    "  memory = ConversationBufferWindowMemory(k=20,\n",
    "                                          return_messages=True)\n",
    "\n",
    "  # Create a conversation chain\n",
    "  conversation_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "      history=RunnableLambda(memory.load_memory_variables)\n",
    "      |\n",
    "      itemgetter(\"history\")\n",
    "    )\n",
    "    |\n",
    "    prompt\n",
    "    |\n",
    "    chatgpt\n",
    "    |\n",
    "    StrOutputParser() # to parse the output to show on UI\n",
    "  )\n",
    "  # Set session variables to be accessed when user enters prompts in the app\n",
    "  cl.user_session.set(\"chain\", conversation_chain)\n",
    "  cl.user_session.set(\"memory\", memory)\n",
    "\n",
    "\n",
    "@cl.on_message\n",
    "# this function is called whenever the user sends a prompt message in the app\n",
    "async def on_user_message(message: cl.Message):\n",
    "\n",
    "  # get the chain and memory objects from the session variables\n",
    "  chain = cl.user_session.get(\"chain\")\n",
    "  memory = cl.user_session.get(\"memory\")\n",
    "\n",
    "  # this will store the response from ChatGPT LLM\n",
    "  chatgpt_message = cl.Message(content=\"\")\n",
    "\n",
    "  # Stream the response from ChatGPT and show in real-time\n",
    "  async for chunk in chain.astream(\n",
    "    {\"input\": message.content},\n",
    "    config=RunnableConfig(callbacks=[cl.LangchainCallbackHandler()]),\n",
    "  ):\n",
    "      await chatgpt_message.stream_token(chunk)\n",
    "  # Finish displaying the full response from ChatGPT\n",
    "  await chatgpt_message.send()\n",
    "  # Store the current conversation in the memory object\n",
    "  memory.save_context({\"input\": message.content},\n",
    "                      {\"output\": chatgpt_message.content})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8de1tM6FVLsq"
   },
   "source": [
    "## Start the app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!chainlit run app.py --port=8989 --watch &>./logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above command doesn't work, use the below command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Za_TAI2RkPI9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-12-14 18:15:46 - Your app is available at http://localhost:8989\n",
      "2024-12-14 18:15:55 - Translated markdown file for en-US not found. Defaulting to chainlit.md.\n",
      "/home/santhosh/Projects/courses/Pinnacle/Building End-to-End Generative AI Applications/Module 3 - Notebooks/app.py:42: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=20,\n",
      "2024-12-14 18:16:13 - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!chainlit run app.py --port=8989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FrVhyQVirAqP",
    "outputId": "28fe76fa-5890-408f-dec5-6ac472255501"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 15:06:27 - Killing ngrok process: 416532\n",
      "2024-07-24 15:06:27 - Updating authtoken for default \"config_path\" of \"ngrok_path\": /home/santhosh/.config/ngrok/ngrok\n",
      "2024-07-24 15:06:27 - Opening tunnel named: http-8989-efdb6608-bc91-4b94-96cb-70a937db1ef3\n",
      "2024-07-24 15:06:28 - t=2024-07-24T15:06:28+0530 lvl=info msg=\"no configuration paths supplied\"\n",
      "2024-07-24 15:06:28 - t=2024-07-24T15:06:28+0530 lvl=info msg=\"using configuration at default config path\" path=/home/santhosh/.config/ngrok/ngrok.yml\n",
      "2024-07-24 15:06:28 - t=2024-07-24T15:06:28+0530 lvl=info msg=\"open config file\" path=/home/santhosh/.config/ngrok/ngrok.yml err=nil\n",
      "2024-07-24 15:06:28 - t=2024-07-24T15:06:28+0530 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=\"client session established\" obj=tunnels.session\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=start pg=/api/tunnels id=8f2f0c7c22a33186\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=end pg=/api/tunnels id=8f2f0c7c22a33186 status=200 dur=373.848Âµs\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=start pg=/api/tunnels id=e185f600b4172f02\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=end pg=/api/tunnels id=e185f600b4172f02 status=200 dur=157.696Âµs\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=start pg=/api/tunnels id=071770ca909ec951\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8989-efdb6608-bc91-4b94-96cb-70a937db1ef3 addr=http://localhost:8989 url=https://83e1-223-190-86-147.ngrok-free.app\n",
      "Chainlit App: https://83e1-223-190-86-147.ngrok-free.app\n",
      "2024-07-24 15:06:30 - t=2024-07-24T15:06:30+0530 lvl=info msg=end pg=/api/tunnels id=071770ca909ec951 status=201 dur=300.092316ms\n",
      "2024-07-24 15:06:40 - t=2024-07-24T15:06:40+0530 lvl=info msg=\"join connections\" obj=join id=0cf70af204d4 l=127.0.0.1:8989 r=223.190.86.147:26776\n",
      "2024-07-24 15:06:41 - t=2024-07-24T15:06:41+0530 lvl=info msg=\"join connections\" obj=join id=b03d9977df03 l=127.0.0.1:8989 r=223.190.86.147:26776\n",
      "2024-07-24 15:06:47 - t=2024-07-24T15:06:47+0530 lvl=info msg=\"join connections\" obj=join id=7525f4012e20 l=127.0.0.1:8989 r=223.190.86.147:31120\n",
      "2024-07-24 15:06:47 - t=2024-07-24T15:06:47+0530 lvl=info msg=\"join connections\" obj=join id=5d15db0bdd82 l=127.0.0.1:8989 r=223.190.86.147:31120\n",
      "2024-07-24 15:06:48 - t=2024-07-24T15:06:48+0530 lvl=info msg=\"join connections\" obj=join id=3a74ae7de05a l=127.0.0.1:8989 r=223.190.86.147:19557\n",
      "2024-07-24 15:06:48 - t=2024-07-24T15:06:48+0530 lvl=info msg=\"join connections\" obj=join id=e33ec1f2d2b4 l=127.0.0.1:8989 r=223.190.86.147:19557\n",
      "2024-07-24 15:06:50 - t=2024-07-24T15:06:50+0530 lvl=info msg=\"join connections\" obj=join id=340480ff714c l=127.0.0.1:8989 r=223.190.86.147:7354\n",
      "2024-07-24 15:07:15 - t=2024-07-24T15:07:15+0530 lvl=info msg=\"join connections\" obj=join id=395cc1e11605 l=127.0.0.1:8989 r=223.190.86.147:19900\n",
      "2024-07-24 15:07:15 - t=2024-07-24T15:07:15+0530 lvl=info msg=\"join connections\" obj=join id=c0b0770e6f3f l=127.0.0.1:8989 r=223.190.86.147:19900\n",
      "2024-07-24 15:07:16 - t=2024-07-24T15:07:16+0530 lvl=info msg=\"join connections\" obj=join id=c47bfa84fc38 l=127.0.0.1:8989 r=223.190.86.147:31372\n",
      "2024-07-24 15:07:30 - t=2024-07-24T15:07:30+0530 lvl=info msg=\"join connections\" obj=join id=7b485b1741de l=127.0.0.1:8989 r=223.190.86.147:13232\n"
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "import yaml\n",
    "\n",
    "# Terminate open tunnels if exist\n",
    "ngrok.kill()\n",
    "\n",
    "# Setting the authtoken\n",
    "# Get your authtoken from `ngrok_credentials.yml` file\n",
    "with open('ngrok_credentials.yml', 'r') as file:\n",
    "    NGROK_AUTH_TOKEN = yaml.safe_load(file)\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN['ngrok_key'])\n",
    "\n",
    "# Open an HTTPs tunnel on port XXXX which you get from your `logs.txt` file\n",
    "ngrok_tunnel = ngrok.connect(8989)\n",
    "print(\"Chainlit App:\", ngrok_tunnel.public_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZndKMZcUVNyi"
   },
   "source": [
    "## Change the Initial app screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ajK7oAJ-nx-",
    "outputId": "bd5b5fac-922d-4e18-b6ad-995e961c06d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting chainlit.md\n"
     ]
    }
   ],
   "source": [
    "%%writefile chainlit.md\n",
    "\n",
    "# Welcome I am AI Assistant ð¤\n",
    "\n",
    "How can I help you today?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q3yFB_jsgC5"
   },
   "source": [
    "## Remove running app processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "pG7Abg_LrAw6"
   },
   "outputs": [],
   "source": [
    "ngrok.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps -ef | grep app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
